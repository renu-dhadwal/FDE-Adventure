---
title: "Level 1 – The Temple of Smriti (Memory)"
format: html
jupyter: python3
---

**Characters**:  
- **Acharya Anvaya (Guide/Teacher, continuity of knowledge)**  
- **Ananya (Learner, seeker of truth)**

---

### Scene: Arrival at the Temple

Ananya and Acharya Anvaya stand before the **Temple of Smriti (Memory)**. Its walls glow faintly with fading carved Sanskrit verses. Above the arch is inscribed:  

*"कालः स्मृतिर्नित्यं — Kālaḥ Smṛtir Nityaṁ (Time is eternal memory)."*

---

## First Steps: Repeated Integration and the Cauchy Formula

**Ananya**: Acharya, this place feels mysterious. What lies in this temple?

**Acharya Anvaya**: This is the Temple of Smriti, or memory. It contains the secrets of memory, the key to which lies in the idea of the *fractional integral*.

**Ananya**: Hmm… the integral is by itself a sort of memory, isn’t it? When we integrate a function from $0$ to $t$:  

$$
I^1 f(t) = \int_{0}^{t} f(\tau)\, d\tau
$$

we are collecting its entire past. This is a kind of memory, isn’t it?

**Acharya Anvaya**: You are absolutely right, Ananya! But before the temple reveals its secret, let us recall what happens when we integrate repeatedly.

**Question**: What is the integral of $1$?

**Ananya**:

$$
I^1(1)(t) = \int_0^t 1\, d\tau = t.
$$

**Acharya Anvaya**: Good. And integrating once more?

**Ananya**:

$$
I^2(1)(t) = \int_0^t I^1(1)(\tau)\, d\tau = \int_0^t \tau\, d\tau = \frac{t^2}{2!}.
$$

**Acharya Anvaya**: And once more?

**Ananya**:

$$
I^3(1)(t) = \int_0^t I^2(1)(\tau)\, d\tau = \int_0^t \frac{\tau^2}{2}\, d\tau = \frac{t^3}{3!}.
$$

**Acharya Anvaya**: Yes! The $n$-th integral of a function $f(t)$ can be written using the *Cauchy formula for repeated integration*:

$$
I^n f(t) = \frac{1}{(n-1)!}\int_0^t (t-\tau)^{\,n-1} f(\tau)\, d\tau.
$$

---

## The Riemann–Liouville Integral

The temple wall shimmers again, and new verses appear, written in flowing Sanskrit:

*"यथा यथा संचितं स्मर्यते, तथा तथैव अनन्तं स्मृतिः — As memory gathers step by step, it stretches into infinity."*

**Acharya Anvaya**: The formula you just derived for integer $n$, Ananya, is not bound to whole numbers alone. The Great ones in modern times, **Riemann from Germany** and **Liouville from France** extended it further. This is known today as the **Riemann–Liouville fractional integral**:

$$
(I^\alpha f)(t) = \frac{1}{\Gamma(\alpha)} \int_0^t (t-\tau)^{\alpha-1} f(\tau)\, d\tau, \qquad \alpha > 0.
$$

**Ananya**: So it is the same as the Cauchy formula, but with $(n-1)!$ replaced by $\Gamma(\alpha)$ — allowing $\alpha$ to be any positive real number?

**Acharya Anvaya**: Exactly. This was the leap — from repeated integer steps to a smooth continuum of memory depths. Each value of $\alpha$ encodes how strongly the past is remembered.


**Acharya Anvaya**: Just as the *smriti* texts preserve memory across centuries, a fractional integral preserves traces of all past values of a function. But the weight of remembrance fades, just as stories fade with time.


**Ananya**: So the past influences the present, but the farther back in time we go, the weaker its pull?

**Acharya Anvaya**: Precisely. Like the echo of a conch in the Himalayas — recent notes ring loud, but the first strike still whispers faintly.

---

### Mini-Quest 1: The Half-Memory

Evaluate $I^{1/2}(1)$.

**Answer**:

$$
(I^{1/2}1)(t) = \frac{2}{\sqrt{\pi}}\, t^{1/2}.
$$

**Acharya Anvaya**: Well done, Ananya. You have grasped the essence of fractional memory. You now hold the **Key of Smriti (Memory)**. With it, the temple doors open to the next journey.

The fractional integral is like oral tradition: newer retellings weigh more, but the first telling is never fully erased.  

A smaller $\alpha$ preserves distant past more strongly — like stories handed down for centuries.  
A larger $\alpha$ emphasizes the near past — like yesterday’s lived experiences.

---

### Mini-Quest 2: Comparing Memories

Which value of $\alpha$ remembers the distant past most strongly?

- Choice A: $\alpha = 0.2$  
- Choice B: $\alpha = 0.5$  
- Choice C: $\alpha = 0.9$

**Ananya**: Since the kernel is $(t-\tau)^{\alpha - 1}$, smaller $\alpha$ means slower decay. So the answer is A.

**Acharya Anvaya**: Correct. You understand how the depth of memory depends on $\alpha$. The temple blesses you with wisdom for the journey ahead.

**Ananya**: I am curious to know more about where such memory comes up.

---

## Example: Stress–Strain in Viscoelastic Fluids

**Acharya Anvaya**: Have you heard of Hooke's law in solids?

**Ananya**: Yes! It says:  
$$
\sigma(t) = E \, \varepsilon(t).
$$  
The stress responds instantly to strain.

**Acharya Anvaya**: But how does this change for flowing materials?

**Ananya**: From what I know, for purely viscous fluids, Newton’s law governs:  
$$
\sigma(t) = \eta \, \frac{d\varepsilon}{dt}(t).
$$

**Acharya Anvaya**: Here stress depends only on the *rate* of strain at the present moment.

**Ananya**: So both Hooke’s law and Newton’s law speak only of the present. They have no memory of what came before?

**Acharya Anvaya**: Exactly. But the world of polymers is different. These are viscoelastic materials. Their stress at time $t$ remembers the entire history of strain. It is written as:

$$
\sigma(t) = \int_0^t G(t-\tau)\,\frac{d\varepsilon}{d\tau}(\tau)\,d\tau,
$$

where $G$ is the **relaxation kernel** — the weight of memory.

**Ananya**: So the kernel is like the echoing chamber of the temple — weighing the voices of the past according to distance in time?

**Acharya Anvaya**: Precisely. If $G$ is exponential we get Maxwell-type models. Experiments often show a power-law:

$$
G(t-\tau)\sim (t-\tau)^{-\alpha}, \qquad 0<\alpha<1.
$$

**Ananya**: Then stress never completely forgets — the echoes fade slowly.

**Acharya Anvaya**: Substituting this kernel gives a fractional constitutive law:

$$
\sigma(t)=\eta_\alpha\,D_t^\alpha\varepsilon(t),
$$

where $D_t^\alpha$ is a **fractional derivative** of the strain.

**Ananya**: But what exactly is a fractional derivative?

**Acharya Anvaya**: For guidance:

- For $\alpha = 1$:  
  $$
  D_t^1 \varepsilon(t) = \frac{d\varepsilon}{dt}(t),
  $$  
  the ordinary derivative, as in Newton’s law of viscosity.

- For $\alpha = 0$:  
  $$
  D_t^0 \varepsilon(t) = \varepsilon(t),
  $$  
  i.e. the function itself, as in Hooke’s law of elasticity.

- For $0<\alpha<1$:  
  $D_t^\alpha$ interpolates smoothly between these two extremes, capturing behavior that is partly elastic and partly viscous (viscoelastic behavior).

**Ananya**: So when $\alpha=0$, the memory is eternal — purely elastic. When $\alpha=1$, memory vanishes quickly — purely viscous. And for $0<\alpha<1$, we have viscoelasticity, with fading memory that bridges the two.

**Acharya Anvaya**: Exactly! Thus, the language of fractional calculus is the scripture that describes memory in matter — echoing from the temple walls into the world of flowing polymers.

**Acharya Anvaya**: We will learn about fractional derivatives as we go further to the Ganges — the River of Derivatives.

**Narrator**: With the Key of Smriti in hand, Ananya steps through the temple gates. The path ahead leads to the **River of Derivatives**.
